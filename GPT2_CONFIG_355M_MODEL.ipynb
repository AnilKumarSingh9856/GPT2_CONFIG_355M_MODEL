{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjUs61b3FSMDXSFIt5BVXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnilKumarSingh9856/GPT2_CONFIG_355M_MODEL/blob/main/GPT2_CONFIG_355M_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPLEMENTING A GPT2_CONFIG_355M MODEL FROM SCRATCH TO GENERATE TEXT"
      ],
      "metadata": {
        "id": "4085xBnPoPLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGkzgZrlnHkc"
      },
      "outputs": [],
      "source": [
        "GPT2_CONFIG_355M = {\n",
        "  \"vocab_size\": 50257,    # Vocabulary size\n",
        "  \"context_length\": 1024, # Context length\n",
        "  \"emb_dim\": 1024,         # Embedding dimension\n",
        "  \"n_heads\": 16,          # Number of attention heads\n",
        "  \"n_layers\": 24,         # Number of layers\n",
        "  \"drop_rate\": 0.1,       # Dropout rate\n",
        "  \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading in a short story as text sample into Python"
      ],
      "metadata": {
        "id": "SO4bmg79smTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"The-Harry-Potter.txt\"\n",
        "url = \"https://raw.githubusercontent.com/AnilKumarSingh9856/Complete_Harry_Potter_txt_file/refs/heads/main/Harry_Potter_complete_dataset.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    text_data = response.read().decode('utf-8')\n",
        "  with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text_data)\n",
        "else:\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()"
      ],
      "metadata": {
        "id": "YdSQ9Xogsl9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print command prints the total number of characters followed by the first 100 characters of this file for illustration purpose"
      ],
      "metadata": {
        "id": "KK0ufmae15kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNkLgULX2NMa",
        "outputId": "d11000b6-5570-4625-f06f-8d71c5d04313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 6285613\n",
            "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly nor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Creating Token IDs by the help of Tiktoken Open AI library"
      ],
      "metadata": {
        "id": "qb3UO9fF2pXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pai40La2QWS",
        "outputId": "886f9a60-5d08-4673-f2a0-a9023d3a167a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv-OgnvE3Afc",
        "outputId": "130c8dd2-a97b-4719-f16b-747cd0651b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:"
      ],
      "metadata": {
        "id": "RzdwDtN03WPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "0qmkEFDm3Kih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "  \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "  \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTMaiv7b3TlO",
        "outputId": "e02c4c61-df92-4cf4-8bb8-19b77ceedd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = tokenizer.decode(integers)"
      ],
      "metadata": {
        "id": "dbMdUxBX38oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugSe9Bdj4X__",
        "outputId": "e1dd402d-d1ca-4f33-9d14-9d16e2b79401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
        "encodings = {\n",
        "  \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
        "  \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
        "  \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
        "}\n",
        "\n",
        "# Get the vocabulary size for each encoding\n",
        "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
        "\n",
        "# Print the vocabulary sizes\n",
        "for model, size in vocab_sizes.items():\n",
        "  print(f\"The vocabulary size for {model.upper()} is: {size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gibxZH4L4Zr5",
        "outputId": "a7671ac4-a120-4277-b8bd-aa260a1c3c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size for GPT2 is: 50257\n",
            "The vocabulary size for GPT3 is: 50281\n",
            "The vocabulary size for GPT4 is: 100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING A DATA LOADER"
      ],
      "metadata": {
        "id": "5f_MnxXQ5aIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and Dataloader classes."
      ],
      "metadata": {
        "id": "po8lr21i5hmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step1: Tokenizer the entire text\n",
        "* Step2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "* Step3: Return the total number of rows in the dataset\n",
        "* Step4: Return a single row from the datset"
      ],
      "metadata": {
        "id": "H8NiRaUp5y51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset:\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    # Tokenizer the entire text\n",
        "    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "    for i in range(0, len(token_ids)-max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      output_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(output_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "VEc3ujbH5Ezi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the corresponding targets.\n",
        "\n",
        "I recommend reading on to see how the data returned from this dataset looks like when we combine the dataset with a PyTorch DataLoader -- this will bring additional intuition and clarity."
      ],
      "metadata": {
        "id": "YlgpDOAZ9d-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will use the GPTDatasetV1 to load the inputs in batches via  a PyTorch DataLoader"
      ],
      "metadata": {
        "id": "hx227lzw9fpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step1: Initialize the tokenizer\n",
        "* Step2: Create dataset\n",
        "* Step3: drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "* Step4: The number of CPU processes to use for preprocessing"
      ],
      "metadata": {
        "id": "bZZUiWX19xzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128,\n",
        "                         shuffle=True, drop_last=True, num_workers=1):\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "  # Create dataset\n",
        "  dataset = GPTDataset(text, tokenizer, max_length, stride)\n",
        "\n",
        "  # Create dataloader\n",
        "  dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    drop_last=drop_last,\n",
        "    num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "SUA7kSEk7aID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function"
      ],
      "metadata": {
        "id": "pNNLEEuR_zBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "  raw_text, batch_size = 8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "intputs_pair, target_pair = next(data_iter)\n",
        "print(f'Input pair \\n{intputs_pair}')\n",
        "print(f'Output pair \\n{target_pair}')"
      ],
      "metadata": {
        "id": "UM9-dPkD_jmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b7c2a0-4feb-40e2-8652-e78af38fb2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input pair \n",
            "tensor([[   44,   374,    13,   290],\n",
            "        [ 9074,    13,   360,  1834],\n",
            "        [ 1636,    11,   286,  1271],\n",
            "        [ 1440,    11,  4389, 16809],\n",
            "        [ 9974,    11,   547,  6613],\n",
            "        [  284,   910,   326,   484],\n",
            "        [  547,  7138,  3487,    11],\n",
            "        [ 5875,   345,   845,   881]])\n",
            "Output pair \n",
            "tensor([[  374,    13,   290,  9074],\n",
            "        [   13,   360,  1834,  1636],\n",
            "        [   11,   286,  1271,  1440],\n",
            "        [   11,  4389, 16809,  9974],\n",
            "        [   11,   547,  6613,   284],\n",
            "        [  910,   326,   484,   547],\n",
            "        [ 7138,  3487,    11,  5875],\n",
            "        [  345,   845,   881,    13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
        "\n",
        "Since the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
        "\n",
        "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256."
      ],
      "metadata": {
        "id": "gvpvEfhWB2Xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Token Embeddings and Positional Embeddings"
      ],
      "metadata": {
        "id": "GhoJTj9qCFo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we focused on very small embedding sizes in this chapter for illustration purposes.\n",
        "\n",
        "We now consider more realistic and useful embedding sizes and encode the input tokens into a 1024-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:"
      ],
      "metadata": {
        "id": "oGNQCmXUCrib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sizes = 50257\n",
        "output_dim = 1024\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_sizes, output_dim)"
      ],
      "metadata": {
        "id": "78faK7DPBs65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 1024-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 1024 tensor."
      ],
      "metadata": {
        "id": "QaBP-9reDSy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate the data loader (Data sampling with a sliding window), first:"
      ],
      "metadata": {
        "id": "yFc9QxAhDdlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "  raw_text, batch_size = 8, max_length=max_length, stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "intputs_pair, target_pair = next(data_iter)\n",
        "print(f'Input pair shape \\n{intputs_pair.shape}')\n",
        "print(f'Output pair shape \\n{target_pair.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6R8ZcwUDOfC",
        "outputId": "9f3ce8c7-b8ab-47c8-f056-5f78cb5a9d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input pair shape \n",
            "torch.Size([8, 4])\n",
            "Output pair shape \n",
            "torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token ID tensor is 8 X 4 dimensional, meaning that the data batch consists of 8 text samples with 4 token each\n",
        "\n",
        "Let's now use the embedding layer to embed these token IDs into 1024-dimensional vectors:"
      ],
      "metadata": {
        "id": "07G07l4XEL_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(intputs_pair)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdfnGNvTEkyT",
        "outputId": "ab7b3e95-2f56-4437-c0a7-ef26f3daf7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding layer, for positions embeddings"
      ],
      "metadata": {
        "id": "e6lvIEuoE318"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "GWQlC6vFEw8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnFPITmwFool",
        "outputId": "ea5aad2d-c9cd-473a-8aff-2fe2debe2eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As we can see, the positional embedding tensor consists of four 1024-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x1024- dimensional pos_embeddings tensor to each 4x1024-dimensional token embedding tensor in each of the 8 batches:"
      ],
      "metadata": {
        "id": "UirDgUMCGaKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings[0]) # printing first batch\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHezRJ94GIly",
        "outputId": "27557d89-c606-4083-c53e-f7c29b7369ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.7253, -0.7782, -0.9055,  ...,  0.3064,  0.2535,  0.8952],\n",
            "        [-0.2899,  1.2833, -0.1203,  ...,  1.1181, -0.7650,  0.0185],\n",
            "        [ 0.8057, -0.6638,  1.1007,  ...,  0.5396,  0.2012, -1.9123],\n",
            "        [-0.5348,  0.1790, -0.1225,  ...,  0.4496,  0.9809, -0.4332]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "torch.Size([8, 4, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS**"
      ],
      "metadata": {
        "id": "UhwzRttZHMvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class.\n",
        "\n",
        "Also, in addition to just merging the MultiHeadAttentionWrapper with the CausalAttention code, we will make some other modifications to implement multi-head attention more efficiently."
      ],
      "metadata": {
        "id": "6ikAgkRFJqr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head.\n",
        "\n",
        "The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated.\n",
        "\n",
        "In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class.\n",
        "\n",
        "It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention."
      ],
      "metadata": {
        "id": "aLw1flMNHISg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the MultiheadAttention class before we discuss it further"
      ],
      "metadata": {
        "id": "fZwh6UfsKGp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads,\n",
        "               qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "      \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                    diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "    # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "    attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "    # Original mask truncated to the number of tokens and converted to boolean\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Use the mask to fill attention scores\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "IVcYQfDdNLSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultiHeadAttention class can be used similar to the SelfAttention and CausalAttention classes we implemented earlier"
      ],
      "metadata": {
        "id": "-9tMczeGPHr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(intputs_pair.shape)\n",
        "batch_size, context_length, d_in = intputs_pair\n",
        "\n"
      ],
      "metadata": {
        "id": "zJ9Ly1o3OAme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}